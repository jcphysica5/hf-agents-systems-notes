{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy Agent Library\n",
    "\n",
    "Building a minimal agent from scratch — no framework, just Python.\n",
    "\n",
    "**Reference:** [HF Agents Course — Unit 1: Dummy Agent Library](https://github.com/huggingface/agents-course/blob/main/units/en/unit1/dummy-agent-library.mdx)\n",
    "\n",
    "---\n",
    "### Setup\n",
    "```\n",
    "pip install huggingface_hub\n",
    "```\n",
    "Set your HF token as the environment variable `HF_TOKEN` (or in Colab → Secrets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b70ac1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "## You need a token from https://hf.co/settings/tokens, ensure that you select 'read' as the token type. If you run this on Google Colab, you can set it up in the \"settings\" tab under \"secrets\". Make sure to call it \"HF_TOKEN\"\n",
    "# HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "client = InferenceClient(model=\"moonshotai/Kimi-K2.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967360d6",
   "metadata": {},
   "source": [
    "& explain here the other models available, why we choose Kimi-K2.5, who is the author of the model, where to acces the list of models&"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077e1458",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb3fb6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "# If running on Colab, load from secrets:\n",
    "# from google.colab import userdata\n",
    "# os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')\n",
    "\n",
    "client = InferenceClient(model=\"moonshotai/Kimi-K2.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Serverless API — quick smoke test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"The capital of France is\"},\n",
    "    ],\n",
    "    stream=False,\n",
    "    max_tokens=32,\n",
    "    extra_body={\"thinking\": {\"type\": \"disabled\"}},\n",
    ")\n",
    "print(output.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. System prompt — tools + ReAct format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"Answer the following questions as best you can. \\\n",
    "You have access to the following tools:\n",
    "\n",
    "get_weather: Get the current weather in a given location\n",
    "\n",
    "The way you use the tools is by specifying a json blob.\n",
    "Specifically, this json should have an `action` key (with the name of the tool to use)\n",
    "and an `action_input` key (with the input to the tool going here).\n",
    "\n",
    "The only values that should be in the \"action\" field are:\n",
    "  get_weather: Get the current weather in a given location,\n",
    "               args: {{\"location\": {{\"type\": \"string\"}}}}\n",
    "\n",
    "example use:\n",
    "  {{ \"action\": \"get_weather\", \"action_input\": {{\"location\": \"New York\"}} }}\n",
    "\n",
    "ALWAYS use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about one action to take. Only one action at a time.\n",
    "Action:\n",
    "```\n",
    "$JSON_BLOB\n",
    "```\n",
    "Observation: the result of the action.\n",
    "... (Thought/Action/Observation can repeat N times)\n",
    "\n",
    "You must always end with:\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Now begin! Reminder to ALWAYS use the exact characters `Final Answer:` when responding.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. First call — spot the hallucination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\",   \"content\": \"What's the weather in London?\"},\n",
    "]\n",
    "\n",
    "output = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    stream=False,\n",
    "    max_tokens=200,\n",
    "    extra_body={\"thinking\": {\"type\": \"disabled\"}},\n",
    ")\n",
    "print(output.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fix: stop before the model invents an Observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    max_tokens=150,\n",
    "    stop=[\"Observation:\"],   # stop here so we can inject the real result\n",
    "    extra_body={\"thinking\": {\"type\": \"disabled\"}},\n",
    ")\n",
    "partial = output.choices[0].message.content\n",
    "print(partial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dummy tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Return fake weather data for a location.\"\"\"\n",
    "    return f\"the weather in {location} is sunny with low temperatures.\\n\"\n",
    "\n",
    "print(get_weather(\"London\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inject real observation and resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_with_obs = [\n",
    "    {\"role\": \"system\",    \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\",      \"content\": \"What's the weather in London?\"},\n",
    "    {\"role\": \"assistant\", \"content\": partial\n",
    "                                     + \"Observation:\\n\"\n",
    "                                     + get_weather(\"London\")},\n",
    "]\n",
    "\n",
    "final_output = client.chat.completions.create(\n",
    "    messages=messages_with_obs,\n",
    "    stream=False,\n",
    "    max_tokens=200,\n",
    "    extra_body={\"thinking\": {\"type\": \"disabled\"}},\n",
    ")\n",
    "print(final_output.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Experiment — try a different city or add a second tool\n",
    "\n",
    "Ideas:\n",
    "- Change the city in the user message\n",
    "- Add a `get_time(city)` tool and update the system prompt\n",
    "- Ask a two-step question that requires both tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your experiments here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf-agents-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
